import pandas as pd
import numpy as np
import os
from collections import defaultdict

from src.load_data.data_load import load_parquet_year_data, load_station_data

BASE_DIR = '.'
DATA_DIR = os.path.join(BASE_DIR, 'data')
OUTPUT_DIR = os.path.join(DATA_DIR, '03')
MASTER_FILE_PATH = os.path.join(DATA_DIR, 'bcycle_master_location.csv')
YEARS_TO_PROCESS = range(2020, 2026)


def load_and_preprocess_master_data():
    """ë§ˆìŠ¤í„° ëŒ€ì—¬ì†Œ ë°ì´í„° ë¡œë“œ ë° ì •ë¦¬"""
    try:
        master_df = load_station_data()
        master_df.dropna(subset=['ëŒ€ì—¬ì†Œ_ID'], inplace=True)
        master_df['ëŒ€ì—¬ì†Œ_ID'] = master_df['ëŒ€ì—¬ì†Œ_ID'].astype(str).str.strip()

        # ì£¼ì†Œ ê²°ì¸¡ì¹˜ ì •ë¦¬
        if 'ì£¼ì†Œ1' not in master_df.columns:
            master_df['ì£¼ì†Œ1'] = ''
        else:
            master_df['ì£¼ì†Œ1'] = master_df['ì£¼ì†Œ1'].fillna('').astype(str).str.strip()

        if 'ì£¼ì†Œ2' not in master_df.columns:
            master_df['ì£¼ì†Œ2'] = ''
        else:
            master_df['ì£¼ì†Œ2'] = master_df['ì£¼ì†Œ2'].fillna('').astype(str).str.strip()

        print(f"âœ… ë§ˆìŠ¤í„° ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(master_df):,}ê°œ ëŒ€ì—¬ì†Œ ì •ë³´")
        return master_df
    except FileNotFoundError:
        print(f"ğŸš¨ ì¹˜ëª…ì  ì˜¤ë¥˜: ë§ˆìŠ¤í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œ: {MASTER_FILE_PATH}")
        return None


def process_raw_data():
    """
    ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì—°ë„ë³„ parquet ë°ì´í„°ë¥¼ ì§‘ê³„.
    - ëŒ€ì—¬ì†Œë³„ ì´ ëŒ€ì—¬/ë°˜ë‚© ê±´ìˆ˜
    - ê²½ë¡œë³„ ì´ìš© ê±´ìˆ˜
    """
    required_columns = ['ì‹œì‘_ëŒ€ì—¬ì†Œ_ID', 'ì¢…ë£Œ_ëŒ€ì—¬ì†Œ_ID', 'ì „ì²´_ê±´ìˆ˜']
    data_generator = load_parquet_year_data(
        selected_years=list(YEARS_TO_PROCESS), columns=required_columns
    )

    rentals = defaultdict(int)
    returns = defaultdict(int)
    routes = defaultdict(int)

    print("ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì›ë³¸ ë°ì´í„° ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    chunk_count = 0
    for chunk_df in data_generator:
        chunk_count += 1
        if chunk_count % 50 == 0:
            print(f"  - {chunk_count}ê°œ ë°ì´í„° ì²­í¬ ì²˜ë¦¬ ì¤‘...")

        chunk_df = chunk_df.loc[:, required_columns].copy()
        chunk_df.dropna(subset=['ì‹œì‘_ëŒ€ì—¬ì†Œ_ID'], inplace=True)
        chunk_df['ì‹œì‘_ëŒ€ì—¬ì†Œ_ID'] = chunk_df['ì‹œì‘_ëŒ€ì—¬ì†Œ_ID'].astype(str).str.strip()
        chunk_df = chunk_df[chunk_df['ì‹œì‘_ëŒ€ì—¬ì†Œ_ID'].str.lower() != 'center']

        # ëŒ€ì—¬ì†Œë³„ ëŒ€ì—¬ ì§‘ê³„
        grp_rent = chunk_df.groupby('ì‹œì‘_ëŒ€ì—¬ì†Œ_ID')['ì „ì²´_ê±´ìˆ˜'].sum()
        for sid, cnt in grp_rent.items():
            rentals[sid] += int(cnt)

        # ë°˜ë‚©/ê²½ë¡œ ì§‘ê³„
        valid_returns = chunk_df[
            chunk_df['ì¢…ë£Œ_ëŒ€ì—¬ì†Œ_ID'].notnull() & (chunk_df['ì¢…ë£Œ_ëŒ€ì—¬ì†Œ_ID'] != 'X')
        ].copy()
        if not valid_returns.empty:
            valid_returns['ì¢…ë£Œ_ëŒ€ì—¬ì†Œ_ID'] = valid_returns['ì¢…ë£Œ_ëŒ€ì—¬ì†Œ_ID'].astype(str).str.strip()
            valid_returns = valid_returns[valid_returns['ì¢…ë£Œ_ëŒ€ì—¬ì†Œ_ID'].str.lower() != 'center']

            grp_ret = valid_returns.groupby('ì¢…ë£Œ_ëŒ€ì—¬ì†Œ_ID')['ì „ì²´_ê±´ìˆ˜'].sum()
            for eid, cnt in grp_ret.items():
                returns[eid] += int(cnt)

            grp_route = valid_returns.groupby(['ì‹œì‘_ëŒ€ì—¬ì†Œ_ID', 'ì¢…ë£Œ_ëŒ€ì—¬ì†Œ_ID'])['ì „ì²´_ê±´ìˆ˜'].sum()
            for (sid, eid), cnt in grp_route.items():
                routes[(sid, eid)] += int(cnt)

    print(f"\nâœ… ì´ {chunk_count:,}ê°œì˜ ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ. ìµœì¢… ë°ì´í„° ì§‘ê³„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")

    if not rentals:
        print("ğŸš¨ ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None, None

    # ê²°ê³¼ DataFrame ìƒì„±
    final_rentals = pd.DataFrame(list(rentals.items()), columns=['ëŒ€ì—¬ì†Œ_ID', 'ì´_ëŒ€ì—¬ê±´ìˆ˜'])
    final_returns = pd.DataFrame(list(returns.items()), columns=['ëŒ€ì—¬ì†Œ_ID', 'ì´_ë°˜ë‚©ê±´ìˆ˜'])
    final_routes = pd.DataFrame(
        [{'ì‹œì‘_ëŒ€ì—¬ì†Œ_ID': s, 'ì¢…ë£Œ_ëŒ€ì—¬ì†Œ_ID': e, 'ì´ìš©_ê±´ìˆ˜': cnt} for (s, e), cnt in routes.items()]
    )

    final_rentals['ì´_ëŒ€ì—¬ê±´ìˆ˜'] = final_rentals['ì´_ëŒ€ì—¬ê±´ìˆ˜'].astype('int64')
    if not final_returns.empty:
        final_returns['ì´_ë°˜ë‚©ê±´ìˆ˜'] = final_returns['ì´_ë°˜ë‚©ê±´ìˆ˜'].astype('int64')
    if not final_routes.empty:
        final_routes['ì´ìš©_ê±´ìˆ˜'] = final_routes['ì´ìš©_ê±´ìˆ˜'].astype('int64')

    return (final_rentals, final_returns), final_routes


def create_station_summary(station_data, master_df):
    """ëŒ€ì—¬ì†Œë³„ ì´ìš© í˜„í™© ìš”ì•½ ìƒì„± (ëŒ€ì—¬ì†Œëª…ì€ ì œì™¸)"""
    final_rentals, final_returns = station_data

    final_rentals['ëŒ€ì—¬ì†Œ_ID'] = final_rentals['ëŒ€ì—¬ì†Œ_ID'].astype(str).str.strip()
    if not final_returns.empty:
        final_returns['ëŒ€ì—¬ì†Œ_ID'] = final_returns['ëŒ€ì—¬ì†Œ_ID'].astype(str).str.strip()
    else:
        final_returns = pd.DataFrame(columns=['ëŒ€ì—¬ì†Œ_ID', 'ì´_ë°˜ë‚©ê±´ìˆ˜'])

    station_summary = pd.merge(final_rentals, final_returns, on='ëŒ€ì—¬ì†Œ_ID', how='outer').fillna(0)
    station_summary['ì´_ì´ìš©ê±´ìˆ˜'] = station_summary['ì´_ëŒ€ì—¬ê±´ìˆ˜'] + station_summary['ì´_ë°˜ë‚©ê±´ìˆ˜']
    station_summary['ìˆœì´ë™ëŸ‰'] = station_summary['ì´_ëŒ€ì—¬ê±´ìˆ˜'] - station_summary['ì´_ë°˜ë‚©ê±´ìˆ˜']

    master_for_merge = master_df[['ëŒ€ì—¬ì†Œ_ID', 'ì£¼ì†Œ1', 'ì£¼ì†Œ2', 'ìœ„ë„', 'ê²½ë„']].copy()
    master_for_merge['ëŒ€ì—¬ì†Œ_ID'] = master_for_merge['ëŒ€ì—¬ì†Œ_ID'].astype(str).str.strip()

    final_station_summary = pd.merge(station_summary, master_for_merge, on='ëŒ€ì—¬ì†Œ_ID', how='inner')

    final_station_summary = final_station_summary[[
        'ëŒ€ì—¬ì†Œ_ID', 'ì£¼ì†Œ1', 'ì£¼ì†Œ2', 'ìœ„ë„', 'ê²½ë„',
        'ì´_ëŒ€ì—¬ê±´ìˆ˜', 'ì´_ë°˜ë‚©ê±´ìˆ˜', 'ì´_ì´ìš©ê±´ìˆ˜', 'ìˆœì´ë™ëŸ‰'
    ]]

    for col in ['ì´_ëŒ€ì—¬ê±´ìˆ˜', 'ì´_ë°˜ë‚©ê±´ìˆ˜', 'ì´_ì´ìš©ê±´ìˆ˜', 'ìˆœì´ë™ëŸ‰']:
        final_station_summary[col] = final_station_summary[col].astype('int64')

    output_path = os.path.join(OUTPUT_DIR, 'station_summary.parquet')
    final_station_summary.to_parquet(output_path, index=False)
    print(f"âœ… ëŒ€ì—¬ì†Œ ìš”ì•½ ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(final_station_summary):,}ê°œ ëŒ€ì—¬ì†Œ ({output_path})")


def create_route_summary(route_data, master_df):
    """ê²½ë¡œë³„ ì´ìš© í˜„í™© ìš”ì•½ ìƒì„± (ëŒ€ì—¬ì†Œëª… ëŒ€ì‹  ì£¼ì†Œ ì‚¬ìš©)"""
    if route_data is None or route_data.empty:
        print("ê²½ë¡œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ìƒëµí•©ë‹ˆë‹¤.")
        return

    route_data = route_data.copy()
    route_data['ì‹œì‘_ëŒ€ì—¬ì†Œ_ID'] = route_data['ì‹œì‘_ëŒ€ì—¬ì†Œ_ID'].astype(str).str.strip()
    route_data['ì¢…ë£Œ_ëŒ€ì—¬ì†Œ_ID'] = route_data['ì¢…ë£Œ_ëŒ€ì—¬ì†Œ_ID'].astype(str).str.strip()

    route_data['ì´ìš©_í˜•íƒœ'] = np.where(
        route_data['ì‹œì‘_ëŒ€ì—¬ì†Œ_ID'] == route_data['ì¢…ë£Œ_ëŒ€ì—¬ì†Œ_ID'],
        'ì™•ë³µ', 'í¸ë„'
    )

    master_info = master_df[['ëŒ€ì—¬ì†Œ_ID', 'ì£¼ì†Œ1', 'ì£¼ì†Œ2', 'ìœ„ë„', 'ê²½ë„']].copy()
    master_info['ëŒ€ì—¬ì†Œ_ID'] = master_info['ëŒ€ì—¬ì†Œ_ID'].astype(str).str.strip()

    final_route_summary = pd.merge(
        route_data, master_info.add_suffix('_ì‹œì‘'),
        left_on='ì‹œì‘_ëŒ€ì—¬ì†Œ_ID', right_on='ëŒ€ì—¬ì†Œ_ID_ì‹œì‘', how='inner'
    )
    final_route_summary = pd.merge(
        final_route_summary, master_info.add_suffix('_ì¢…ë£Œ'),
        left_on='ì¢…ë£Œ_ëŒ€ì—¬ì†Œ_ID', right_on='ëŒ€ì—¬ì†Œ_ID_ì¢…ë£Œ', how='inner'
    )

    final_route_summary = final_route_summary[[
        'ì‹œì‘_ëŒ€ì—¬ì†Œ_ID', 'ì¢…ë£Œ_ëŒ€ì—¬ì†Œ_ID', 'ì´ìš©_ê±´ìˆ˜', 'ì´ìš©_í˜•íƒœ',
        'ì£¼ì†Œ1_ì‹œì‘', 'ì£¼ì†Œ2_ì‹œì‘', 'ìœ„ë„_ì‹œì‘', 'ê²½ë„_ì‹œì‘',
        'ì£¼ì†Œ1_ì¢…ë£Œ', 'ì£¼ì†Œ2_ì¢…ë£Œ', 'ìœ„ë„_ì¢…ë£Œ', 'ê²½ë„_ì¢…ë£Œ'
    ]]

    output_path = os.path.join(OUTPUT_DIR, 'route_summary.parquet')
    final_route_summary.to_parquet(output_path, index=False)
    print(f"âœ… ê²½ë¡œ ìš”ì•½ ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(final_route_summary):,}ê°œ ê²½ë¡œ ({output_path})")


if __name__ == '__main__':
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    master_df = load_and_preprocess_master_data()

    if master_df is not None:
        station_data, route_data = process_raw_data()

        if station_data and route_data is not None:
            create_station_summary(station_data, master_df)
            create_route_summary(route_data, master_df)
            print("\nğŸ‰ ëª¨ë“  ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")